{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6TuY1JmJKE"
      },
      "source": [
        "# **Gemma を使ったファインチューニング入門**\n",
        "本ハンズオンでは、[日本語版 Gemma 2 2B](https://developers-jp.googleblog.com/2024/10/gemma-2-for-japan.html) (`gemma-2-2b-jpn`) と [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) 及び [Hugging Face TRL](https://huggingface.co/docs/trl/en/index) を使って、[LoRA](https://arxiv.org/abs/2106.09685) によるファインチューニングを行います。\n",
        "\n",
        "前提条件: 以下の作業が完了していること\n",
        "* Hugging Face アカウントの作成\n",
        "* Hugging Face Access Token の発行\n",
        "* Gemma の利用規約への同意\n",
        "\n",
        "推奨ランタイム:\n",
        "*    g2-standard-12 (NVIDIA L4 * 1) もしくは a2-highgpu-1g (NVIDIA A100 * 1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUHRBfMjsmUy"
      },
      "source": [
        "# **1. 環境準備**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzIacm188Ets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a51iJNxXmJKH"
      },
      "outputs": [],
      "source": [
        "# @title 1-1. 必要パッケージをインストール\n",
        "# @markdown [補足] Hugging Face 関連\n",
        "\n",
        "# @markdown - `transformers` は、生成 AI や自然言語処理 (NLP) タスク用のモデルを簡単にダウンロード、トレーニング、および使用できるようにするパッケージ\n",
        "\n",
        "# @markdown - `accelerate` はモデルのトレーニングや推論を、CPU、GPU、TPUなど多様なデバイスや分散環境で効率的に実行するためのパッケージ\n",
        "\n",
        "# @markdown - `datasets` は Hugging Face で公開されている様々なデータセットを簡単にダウンロード、前処理、操作できるパッケージ\n",
        "\n",
        "# @markdown - `peft` その名の通り PEFT を行うためのパッケージ\n",
        "\n",
        "# @markdown - `trl` トランスフォーマーモデルを SFT や RLHF でトレーニングするためのフルスタック パッケージ\n",
        "\n",
        "# @markdown [補足] PyTorch 関連\n",
        "\n",
        "# @markdown - `torch` (PyTorch) は 深層学習フレームワークで、Hugging Face Transformers のバックエンドとして GPU や LoRA の重み更新などを実際にサポートしているパッケージ\n",
        "\n",
        "# @markdown - `tensorboard` は、ファインチューニング中に出力されるログを可視化するパッケージ\n",
        "\n",
        "# Pytorch 関連のパッケージをインストール\n",
        "!pip install --upgrade torch tensorboard\n",
        "\n",
        "# Hugging Face 関連のパッケージをインストール\n",
        "!pip install --upgrade \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  datasets \\\n",
        "  peft \\\n",
        "  trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SRnxcebsenP"
      },
      "outputs": [],
      "source": [
        "# @title 1-2. Hugging Face の Access Token を変数に代入\n",
        "# @markdown [補足]\n",
        "\n",
        "# @markdown - Hugging Face から Gemma モデルをダウンロードする時や、ファインチューニングしたモデルを Hugging Face にプッシュする時に使います\n",
        "hf_access_token = \"\" # @param {\"type\":\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ntaxbPXmmJKI"
      },
      "outputs": [],
      "source": [
        "# @title 1-3. Hugging Face にログイン\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=hf_access_token,\n",
        "  add_to_git_credential=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFCp4zysrM83"
      },
      "source": [
        "# **2. `gemma-2-2b-int-jp` を使用**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G-LGLcsmw0X2"
      },
      "outputs": [],
      "source": [
        "# @title 2-1. トークナイザーとモデルの読み込み\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Hugging Face 上で有効なモデルの ID を変数に代入\n",
        "model_id = \"google/gemma-2-2b-jpn-it\"\n",
        "\n",
        "# トークナイザーとモデルの読み込み\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DJBExYmgD6w"
      },
      "outputs": [],
      "source": [
        "# @title 2-2. プロンプトの作成と推論、結果確認\n",
        "# Gemma に聞く質問を定義\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"日本の首都はどこですか？\"},\n",
        "]\n",
        "\n",
        "# Gemma への入力用に Messages をトークン化\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True).to(model.device)\n",
        "\n",
        "# Gemma による生成と返ってきたトークンを自然言語にデコード\n",
        "outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "generated_text = tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0]\n",
        "gemma_response1 = generated_text.strip()\n",
        "\n",
        "print(gemma_response1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlJGqe9mm67"
      },
      "outputs": [],
      "source": [
        "# @title 2-3. メモリを開放\n",
        "del model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGWWXf6tmJKI"
      },
      "source": [
        "# **3. ファインチューニング用のデータセットを準備**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tHmi-727xrm"
      },
      "outputs": [],
      "source": [
        "# @markdown [補足]\n",
        "\n",
        "# @markdown - 本デモでは以下のデータセットを使用しています：\n",
        "# @markdown - データセット名: bbz662bbz/databricks-dolly-15k-ja-gozaru\n",
        "# @markdown - データセットの提供元: bbz662bbz\n",
        "# @markdown - ライセンス: CC BY-SA 3.0\n",
        "# @markdown - https://huggingface.co/datasets/bbz662bbz/databricks-dolly-15k-ja-gozaru\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# データセットの読み込み\n",
        "dataset = load_dataset(\"bbz662bbz/databricks-dolly-15k-ja-gozaru\", split=\"train\")\n",
        "dataset = dataset.filter(lambda example: example[\"category\"] == \"open_qa\")\n",
        "\n",
        "# データセットをプロンプト形式に変換する関数の定義\n",
        "def generate_prompt(example):\n",
        "    return \"\"\"<bos><start_of_turn>user\n",
        "{}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{}<eos>\"\"\".format(example[\"instruction\"], example[\"output\"])\n",
        "\n",
        "# textカラムの追加\n",
        "def add_text(example):\n",
        "    example[\"text\"] = generate_prompt(example)\n",
        "    return example\n",
        "dataset = dataset.map(add_text)\n",
        "dataset = dataset.remove_columns([\"input\", \"category\", \"output\", \"index\", \"instruction\"])\n",
        "\n",
        "# データセットの分割\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# 最終的なデータセットの確認\n",
        "print(len(train_dataset))\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKlEnozbmJKJ"
      },
      "source": [
        "## **4. `TRL` と `SFTTrainer` を使ったトレーニグ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ELcOTtMmJKK"
      },
      "outputs": [],
      "source": [
        "# @title 4-1. ファインチューニング用にトークナイザーとモデルの読み込み\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from trl import setup_chat_format\n",
        "\n",
        "# Hugging Face 上で有効なモデルの ID を指定\n",
        "model_id = \"google/gemma-2-2b-jpn-it\"\n",
        "\n",
        "# トークナイザーとモデルの読み込み\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# 警告を回避するためトークナイザーのパdディング方向を右側に設定\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCyxa3p2mJKK"
      },
      "outputs": [],
      "source": [
        "# @title 4-2. LoRA の設定\n",
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=32,         # LoRAのスケーリング係数（大きいほど更新の影響が大きくなる）\n",
        "        lora_dropout=0.1,      # ドロップアウト率、LoRAのパラメータ更新時の過学習を防ぐ\n",
        "        r=8,                   # LoRAでのランク（低ランク行列の次元）、モデルの容量と精度のバランスに影響\n",
        "        bias=\"none\",           # バイアスの学習方法（noneはバイアスを更新しない設定）\n",
        "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # LoRAを適用する対象モジュール（プロジェクション層や特定のネットワーク層）\n",
        "        task_type=\"CAUSAL_LM\", # タスクの種類、ここでは自己回帰型言語モデル (Causal Language Modeling) を指定\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZCA9nljmJKK"
      },
      "outputs": [],
      "source": [
        "# @title 4-3. ハイパーパラメータの設定\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"gemma-2-2b-int-jpn-lora1\",  # モデルを保存するディレクトリとリポジトリID\n",
        "    num_train_epochs=3,                     # 学習エポック数\n",
        "    per_device_train_batch_size=1,          # デバイスごとの学習バッチサイズ\n",
        "    gradient_accumulation_steps=2,          # バックワード/更新を行う前のステップ数\n",
        "    gradient_checkpointing=True,            # メモリ節約のために勾配チェックポイントを使用\n",
        "    optim=\"adamw_torch_fused\",              # 融合AdamWオプティマイザを使用\n",
        "    logging_steps=10,                       # 10ステップごとにログ出力\n",
        "    save_strategy=\"epoch\",                  # 各エポック終了時にチェックポイントを保存\n",
        "    learning_rate=2e-4,                     # 学習率（QLoRA論文に基づく）\n",
        "    bf16=True,                              # 対応GPUがある場合にbfloat16精度を使用\n",
        "    tf32=True,                              # 対応GPUがある場合にtf32精度を使用\n",
        "    max_grad_norm=0.3,                      # 最大勾配ノルム（QLoRA論文に基づく）\n",
        "    warmup_ratio=0.03,                      # ウォームアップ比率（QLoRA論文に基づく）\n",
        "    lr_scheduler_type=\"constant\",           # 定数学習率スケジューラを使用\n",
        "    push_to_hub=True,                       # モデルをHugging Face Hubにプッシュ\n",
        "    report_to=\"tensorboard\",                # メトリクスをtensorboardにレポート\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxxvp94hmJKK"
      },
      "outputs": [],
      "source": [
        "# @title 4-4. SFTTrainer クラスのインスタンスを作成\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# モデルに入力できるテキストの最大長を 1024トークンに制限\n",
        "max_seq_length = 1024\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "     dataset_text_field=\"text\",\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U02TPKHPmJKL"
      },
      "outputs": [],
      "source": [
        "# @title 4-5. トレーニングの実行\n",
        "from datetime import datetime\n",
        "\n",
        "# トレーニング開始前に現在時刻を出力\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Training started at: {current_time}\")\n",
        "\n",
        "# トレーニングを実行開始\n",
        "trainer.train()\n",
        "\n",
        "# トレーニングが完了したモデルを保存\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ5O_AEWmJKL"
      },
      "outputs": [],
      "source": [
        "# @title 4-6. メモリの開放\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihxnz9kdmJKL"
      },
      "outputs": [],
      "source": [
        "# @title 4-7. LoRA adapter とオリジナルモデルをマージ\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# Load PEFT model on CPU\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    args.output_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "    # low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Merge LoRA and base model and save\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOjv6NTMmJKL"
      },
      "source": [
        "## **5. ファインチューニング後のモデルをテスト**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uMuQ-DsmJKL"
      },
      "outputs": [],
      "source": [
        "# @title 5-1. トークナイザーとモデルの読み込み\n",
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = args.output_dir\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C9T4E0JmJKL"
      },
      "outputs": [],
      "source": [
        "# @title 5-2. プロンプトの作成と推論、結果確認\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"日本の首都はどこですか？\" },\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True).to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "generated_text = tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0]\n",
        "gemma_response2 = generated_text.strip()\n",
        "\n",
        "print(gemma_response2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "name": "Gemma ファインチューニング入門.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}